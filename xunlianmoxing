import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.callbacks import ModelCheckpoint

# 构建LSTM模型
def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    model = Sequential([
        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),
        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
        Dense(vocab_size)
    ])
    return model

# 训练模型
def train_model(text, epochs=50, batch_size=64):
    words = preprocess(text)
    word_to_id = {word: index for index, word in enumerate(sorted(set(words)))}
    id_to_word = {index: word for word, index in word_to_id.items()}
    vocab_size = len(word_to_id)
    word_ids = [word_to_id[word] for word in words]
    dataset = tf.data.Dataset.from_tensor_slices(word_ids)
    sequences = dataset.batch(batch_size=batch_size, drop_remainder=True)
    model = build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=batch_size)
    model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
    checkpoint_prefix = './checkpoints/{epoch:02d}.hdf5'
    checkpoint_callback = ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)
    model.fit(sequences, epochs=epochs, callbacks=[checkpoint_callback])
    return model, id_to_word
